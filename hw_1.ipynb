{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem set 1, Intro to NLP 2018\n",
    "\n",
    "#### This is due on September 25, 2018, submitted electronically. 100 points total.\n",
    "\n",
    "##### How to do this problem set:\n",
    "\n",
    "- What version of Python should I use? 3.6!\n",
    "\n",
    "- Most of these questions require writing Python code and computing results, and the rest of them have textual answers. To generate the answers, you will have to fill out a supporting file, `hw1.py`.\n",
    "\n",
    "- For all of the textual answers you have to fill out have placeholder text which says \"Answer in one or two sentences here.\" For each question, you need to replace \"Answer in one or two sentences here\" with your answer.\n",
    "\n",
    "- Write all the answers in this ipython notebook. Once you are finished (1) Generate a PDF via (File -> Download As -> PDF) and upload to Gradescope (2)Turn in `hw_1.py` and `hw_1.ipynb` on Moodle.\n",
    "  \n",
    "- **Important** check your PDF before you turn it in to gradescope to make sure it exported correctly. If ipython notebook gets confused about your syntax it will sometimes terminate the PDF creation routine early. If your whole PDF does not print, try running `$jupyter nbconvert --to pdf 2018hw1.ipynb` to identify and fix any syntax errors that might be causing problems\n",
    "\n",
    "- When creating your final version of the PDF to hand in, please do a fresh restart and execute every cell in order. Then you'll be sure it's actually right. One handy way to do this is by clicking `Cell -> Run All` in the notebook menu.\n",
    " \n",
    "- This assignment is designed so that you can run all cells in a few minutes of computation time. If it is taking longer than that, you probably have made a mistake in your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Academic honesty \n",
    "\n",
    "- We will audit the Moodle code from a set number of students, chosen at random. The audits will check that the code you wrote and turned on Moodle generates the answers you turn in on your PDF. If you turn in correct answers on your PDF without code that actually generates those answers, we will consider this a serious case of cheating. See the course page for honesty policies.\n",
    "\n",
    "- We will also run automatic checks of code on Moodle for plagiarism. Copying code from others is also considered a serious case of cheating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Run this cell! It sets some things up for you.\n",
    "\n",
    "# This code makes plots appear inline in this document rather than in a new window.\n",
    "import matplotlib.pyplot as plt\n",
    "from __future__ import division  # this line is important to avoid unexpected behavior from division\n",
    "\n",
    "# This code imports your work from hw_1.py\n",
    "from model import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5, 4) # set default size of plots\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "popular_rap\n",
      "['childish+gambino-freaks+and+geeks.txt', 'lil+pump-gucci+gang.txt', 'kanye+west-blood+on+the+leaves.txt', 'lil+pump-flex+like+ouu.txt', 'lil+pump-drug+addict.txt', 'eminem-renegade.txt', 'tyler+the+creator-911+mr+lonely.txt', 'aer-above+my+floor.txt', 'kendrick+lamar-humble.txt', '.DS_Store', 'chance+the+rapper-no+problem.txt', 'jay+z-99+problems.txt', 'meek+mill-on+me.txt', 'kendrick+lamar-swimming+pools.txt', 'nicki+minaj-chun+li.txt', 'nwa-chin+check.txt', 'kodak+black-calling+my+spirit.txt', 'aer-pretty+lady+around+me.txt', 'travis+scott-sicko+mode.txt', 'lil+pump-boss.txt', 'nicki+minaj-barbie+tingz.txt', 'tyler+the+creator-who+dat+boy+feat+aap+rocky.txt', 'kanye+west-famous.txt', 'chance+the+rapper-my+own+thing.txt', 'tyler+the+creator-see+you+again.txt', 'aer-whatever+we+want.txt', 'kanye+west-gold+digger.txt', 'drake-nonstop.txt', 'aer-wont+laugh.txt', '6ix9ine-tic+toc.txt', 'chance+the+rapper-all+we+got.txt', 'tyler+the+creator-boredom.txt', 'childish+gambino-california.txt', 'aer-floats+my+boat.txt', 'iggy+azalea-black+widow.txt', 'lil+pump-jettski+grizzley.txt', 'lil+pump-esskeetit.txt', 'aer-come+go.txt']\n",
      "['childish', 'gambino-freaks', 'and', 'geeks.txt']\n",
      "['lil', 'pump-gucci', 'gang.txt']\n",
      "['kanye', 'west-blood', 'on', 'the', 'leaves.txt']\n",
      "['lil', 'pump-flex', 'like', 'ouu.txt']\n",
      "['lil', 'pump-drug', 'addict.txt']\n",
      "['eminem-renegade.txt']\n",
      "['tyler', 'the', 'creator-911', 'mr', 'lonely.txt']\n",
      "['aer-above', 'my', 'floor.txt']\n",
      "['kendrick', 'lamar-humble.txt']\n",
      "['.DS_Store']\n",
      "['chance', 'the', 'rapper-no', 'problem.txt']\n",
      "['jay', 'z-99', 'problems.txt']\n",
      "['meek', 'mill-on', 'me.txt']\n",
      "['kendrick', 'lamar-swimming', 'pools.txt']\n",
      "['nicki', 'minaj-chun', 'li.txt']\n",
      "['nwa-chin', 'check.txt']\n",
      "['kodak', 'black-calling', 'my', 'spirit.txt']\n",
      "['aer-pretty', 'lady', 'around', 'me.txt']\n",
      "['travis', 'scott-sicko', 'mode.txt']\n",
      "['lil', 'pump-boss.txt']\n",
      "['nicki', 'minaj-barbie', 'tingz.txt']\n",
      "['tyler', 'the', 'creator-who', 'dat', 'boy', 'feat', 'aap', 'rocky.txt']\n",
      "['kanye', 'west-famous.txt']\n",
      "['chance', 'the', 'rapper-my', 'own', 'thing.txt']\n",
      "['tyler', 'the', 'creator-see', 'you', 'again.txt']\n",
      "['aer-whatever', 'we', 'want.txt']\n",
      "['kanye', 'west-gold', 'digger.txt']\n",
      "['drake-nonstop.txt']\n",
      "['aer-wont', 'laugh.txt']\n",
      "['6ix9ine-tic', 'toc.txt']\n",
      "['chance', 'the', 'rapper-all', 'we', 'got.txt']\n",
      "['tyler', 'the', 'creator-boredom.txt']\n",
      "['childish', 'gambino-california.txt']\n",
      "['aer-floats', 'my', 'boat.txt']\n",
      "['iggy', 'azalea-black', 'widow.txt']\n",
      "['lil', 'pump-jettski', 'grizzley.txt']\n",
      "['lil', 'pump-esskeetit.txt']\n",
      "['aer-come', 'go.txt']\n",
      "Oh no! Something is wrong. Check your code which loads the reviews\n",
      "unpopular_rap\n",
      "['towkio-cntrl.txt', 'the+deans+list-kryptonite+sanity+room.txt', 'aer-go.txt', 'tayyib+ali-like+i+used+to.txt', 'oncue-time.txt', 'roc+marciano-burkina+faso.txt', 'binary+star-i+know+why+the+caged+bird+sings+part+1.txt', 'busdriver-casting+agents+and+cowgirls.txt', '.DS_Store', 'roc+marciano-death+parade.txt', 'aer-stars.txt', 'roc+marciano-corniche.txt', 'towkio-alone.txt', 'roc+marciano-cvs.txt', 'yonas-stand+out.txt', 'chris+webby-blunt+ride+cypher.txt', 'yonas-nobody+else.txt', 'roc+marciano-confucius.txt', 'binary+star-conquistadors.txt', 'towkio-swim.txt', 'oncue-this+is+not+a+song.txt', 'roc+marciano-bruh+man.txt', 'kevin+abstract-suburbian+born.txt', 'chris+webby-through+the+roof.txt', 'aer-im+not+sorry.txt', 'blue+scholars-blink.txt', 'roc+marciano-bedspring+king.txt', 'aer-im+with+it.txt', 'binary+star-solar+powered.txt', 'nujabes-perfect+circle.txt', 'yonas-mindless.txt', 'kinetics+one+love-still+dreamin.txt', 'towkio-tear+drop.txt', 'hendersin-kids.txt', 'roc+marciano-cut+the+check.txt', 'chris+webby-house+party+cypher+interlude.txt', 'chris+webby-nice+2+be+back.txt', 'chris+webby-take+me+home.txt', 'chris+webby-brim+low.txt', 'roc+marciano-already.txt', 'yonas-yall+know.txt', 'binary+star-indy+500.txt', 'chris+webby-label+office+cypher.txt', 'chris+webby-stand+up.txt', 'hendersin-fear.txt', 'immortal+technique-crimes+of+the+heart.txt', 'towkio-disco.txt', 'roc+marciano-congo.txt', 'roc+marciano-20+guns.txt', 'busdriver-avantcore.txt', 'towkio-hate+to+love.txt', 'roc+marciano-chewbacca.txt', 'tayyib+ali-i+want+it+all.txt', 'binary+star-honest+expression.txt', 'roc+marciano-love+means.txt', 'aer-ex.txt', 'roc+marciano-death+sentence.txt', 'towkio-forever.txt', 'nliten-lite+it+up.txt', 'blue+scholars-burnt+offering.txt', 'oncue-cereal.txt', 'ollie-spring+is+here.txt', 'blue+scholars-selfportrait.txt', 'towkio-loose.txt', 'binary+star-masters+of+the+universe.txt', 'packy-wobbin.txt', 'binary+star-evolution+of+man.txt', 'busdriver-rapper.txt', 'blue+scholars-life+debt.txt', 'binary+star-glen+close.txt', 'towkio-lose+me+i+dont+mind.txt']\n",
      "Oh no! Something is wrong. Check your code which loads the reviews\n"
     ]
    }
   ],
   "source": [
    "PATH_TO_DATA = 'lyrics'\n",
    "\n",
    "RAP_POS_LABEL = 'popular_rap'\n",
    "RAP_NEG_LABEL = 'unpopular_rap'\n",
    "COUNTRY_POS_LABEL = 'popular_country'\n",
    "COUNTRY_NEG_LABEL = 'unpopular_country'\n",
    "\n",
    "RAP_DIR = os.path.join(PATH_TO_DATA, \"rap\")\n",
    "COUNTRY_DIR = os.path.join(PATH_TO_DATA, \"country\")\n",
    "\n",
    "RAP_TRAIN_DIR = os.path.join(RAP_DIR, \"train\")\n",
    "RAP_TEST_DIR = os.path.join(RAP_DIR, \"test\")\n",
    "COUNTRY_TRAIN_DIR = os.path.join(COUNTRY_DIR, \"train\")\n",
    "COUNTRY_TEST_DIR = os.path.join(COUNTRY_DIR, \"test\")\n",
    "\n",
    "for label in [RAP_POS_LABEL, RAP_NEG_LABEL]:\n",
    "    print(label)\n",
    "    if label == RAP_POS_LABEL:\n",
    "        print(os.listdir(RAP_TRAIN_DIR + \"/\" + label))\n",
    "        for file in os.listdir(RAP_TRAIN_DIR + \"/\" + label):\n",
    "            string = file.split('+')\n",
    "            print(string)\n",
    "        if len(os.listdir(RAP_TRAIN_DIR + \"/\" + label)) == 12:\n",
    "            print(\"Great! You have 12 {} reviews in {}\".format(label, RAP_TRAIN_DIR + \"/\" + label))\n",
    "        else:\n",
    "            print(\"Oh no! Something is wrong. Check your code which loads the reviews\")\n",
    "    else:\n",
    "        print(os.listdir(RAP_TRAIN_DIR + \"/\" + label))\n",
    "        if len(os.listdir(RAP_TRAIN_DIR + \"/\" + label)) == 7:\n",
    "            print(\"Great! You have 7 {} reviews in {}\".format(label, RAP_TRAIN_DIR + \"/\" + label))\n",
    "        else:\n",
    "            print(\"Oh no! Something is wrong. Check your code which loads the reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TRAIN_DIR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-8ed51a7f043a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Actually reading the data you are working with is an important part of NLP! Let's look at one of these reviews\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_DIR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/neg/98_1.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'TRAIN_DIR' is not defined"
     ]
    }
   ],
   "source": [
    "# Actually reading the data you are working with is an important part of NLP! Let's look at one of these reviews\n",
    "\n",
    "print (open(TRAIN_DIR + \"/neg/98_1.txt\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part One: Intro to NLP in Python: types, tokens and Zipf's law"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Types and tokens\n",
    "\n",
    "One major part of any NLP project is word tokenization. Word tokenization is the task of segmenting text into individual words, called tokens. In this assignment, we will use simple whitespace tokenization. Take a look at the `tokenize_doc` function in `hw_1.py`. **You should not modify tokenize_doc** but make sure you understand what it is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmpsci\n",
      "585\n",
      "is\n",
      "already\n",
      "my\n",
      "favorite\n",
      "class\n",
      "this\n",
      "semester!\n"
     ]
    }
   ],
   "source": [
    "# We have provided a tokenize_doc function in hw_1.py. Here is a short demo of how it works\n",
    "\n",
    "d1 = \"This SAMPLE doc has   words tHat  repeat repeat\"\n",
    "bow = tokenize_doc(d1)\n",
    "\n",
    "assert bow['this'] == 1\n",
    "assert bow['sample'] == 1\n",
    "assert bow['doc'] == 1\n",
    "assert bow['has'] == 1\n",
    "assert bow['words'] == 1\n",
    "assert bow['that'] == 1\n",
    "assert bow['repeat'] == 2\n",
    "\n",
    "bow2 = tokenize_doc(\"CMPSCI 585 is already my favorite class this semester!\")\n",
    "for b in bow2:\n",
    "    print (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to look at the word types and word tokens in the corpus.\n",
    "Use the `word_counts` dictionary variable to store the count of each word in the corpus.\n",
    "Use the `tokenize_doc` function to break documents into tokens. **You should not modify tokenize_doc** but make sure you understand what it is doing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.1 (5 points)**\n",
    "\n",
    "Complete the cell below to fill out the `word_counts` dictionary variable. `word_counts` keeps track of how many times a word type appears across the corpus. For instance, `word_counts[\"movie\"]` should store the number 61492 -- the count of how many times the word `movie` appears in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lyrics/rap/train/popular_rap/childish+gambino-freaks+and+geeks.txt\n",
      "lyrics/rap/train/popular_rap/lil+pump-gucci+gang.txt\n",
      "lyrics/rap/train/popular_rap/kanye+west-blood+on+the+leaves.txt\n",
      "lyrics/rap/train/popular_rap/lil+pump-flex+like+ouu.txt\n",
      "lyrics/rap/train/popular_rap/lil+pump-drug+addict.txt\n",
      "lyrics/rap/train/popular_rap/eminem-renegade.txt\n",
      "lyrics/rap/train/popular_rap/tyler+the+creator-911+mr+lonely.txt\n",
      "lyrics/rap/train/popular_rap/aer-above+my+floor.txt\n",
      "lyrics/rap/train/popular_rap/kendrick+lamar-humble.txt\n",
      "lyrics/rap/train/popular_rap/chance+the+rapper-no+problem.txt\n",
      "lyrics/rap/train/popular_rap/jay+z-99+problems.txt\n",
      "lyrics/rap/train/popular_rap/meek+mill-on+me.txt\n",
      "lyrics/rap/train/popular_rap/kendrick+lamar-swimming+pools.txt\n",
      "lyrics/rap/train/popular_rap/nicki+minaj-chun+li.txt\n",
      "lyrics/rap/train/popular_rap/nwa-chin+check.txt\n",
      "lyrics/rap/train/popular_rap/kodak+black-calling+my+spirit.txt\n",
      "lyrics/rap/train/popular_rap/aer-pretty+lady+around+me.txt\n",
      "lyrics/rap/train/popular_rap/travis+scott-sicko+mode.txt\n",
      "lyrics/rap/train/popular_rap/lil+pump-boss.txt\n",
      "lyrics/rap/train/popular_rap/nicki+minaj-barbie+tingz.txt\n",
      "lyrics/rap/train/popular_rap/tyler+the+creator-who+dat+boy+feat+aap+rocky.txt\n",
      "lyrics/rap/train/popular_rap/kanye+west-famous.txt\n",
      "lyrics/rap/train/popular_rap/chance+the+rapper-my+own+thing.txt\n",
      "lyrics/rap/train/popular_rap/tyler+the+creator-see+you+again.txt\n",
      "lyrics/rap/train/popular_rap/aer-whatever+we+want.txt\n",
      "lyrics/rap/train/popular_rap/kanye+west-gold+digger.txt\n",
      "lyrics/rap/train/popular_rap/drake-nonstop.txt\n",
      "lyrics/rap/train/popular_rap/aer-wont+laugh.txt\n",
      "lyrics/rap/train/popular_rap/6ix9ine-tic+toc.txt\n",
      "lyrics/rap/train/popular_rap/chance+the+rapper-all+we+got.txt\n",
      "lyrics/rap/train/popular_rap/tyler+the+creator-boredom.txt\n",
      "lyrics/rap/train/popular_rap/childish+gambino-california.txt\n",
      "lyrics/rap/train/popular_rap/aer-floats+my+boat.txt\n",
      "lyrics/rap/train/popular_rap/iggy+azalea-black+widow.txt\n",
      "lyrics/rap/train/popular_rap/lil+pump-jettski+grizzley.txt\n",
      "lyrics/rap/train/popular_rap/lil+pump-esskeetit.txt\n",
      "lyrics/rap/train/popular_rap/aer-come+go.txt\n",
      "lyrics/rap/test/popular_rap/nwa-boyz+n+the+hood.txt\n",
      "lyrics/rap/test/popular_rap/childish+gambino-redbone.txt\n",
      "lyrics/rap/test/popular_rap/chance+the+rapper-acid+rain.txt\n",
      "lyrics/rap/test/popular_rap/jay+z-big+pimpin.txt\n",
      "lyrics/rap/test/popular_rap/nwa-fuck+tha+police.txt\n",
      "lyrics/rap/test/popular_rap/kendrick+lamar-king+kunta.txt\n",
      "lyrics/rap/test/popular_rap/iggy+azalea-100.txt\n",
      "lyrics/rap/test/popular_rap/chance+the+rapper-same+drugs.txt\n",
      "lyrics/rap/test/popular_rap/iggy+azalea-just+asking.txt\n",
      "lyrics/rap/test/popular_rap/kendrick+lamar-no+makeup.txt\n",
      "lyrics/rap/test/popular_rap/kanye+west-big+brother.txt\n",
      "lyrics/rap/test/popular_rap/ice+cube-it+was+a+good+day.txt\n",
      "lyrics/rap/test/popular_rap/childish+gambino-3005.txt\n",
      "lyrics/rap/test/popular_rap/iggy+azalea-fancy.txt\n",
      "lyrics/rap/test/popular_rap/kanye+west-good+life.txt\n",
      "lyrics/rap/train/unpopular_rap/towkio-cntrl.txt\n",
      "lyrics/rap/train/unpopular_rap/the+deans+list-kryptonite+sanity+room.txt\n",
      "lyrics/rap/train/unpopular_rap/aer-go.txt\n",
      "lyrics/rap/train/unpopular_rap/tayyib+ali-like+i+used+to.txt\n",
      "lyrics/rap/train/unpopular_rap/oncue-time.txt\n",
      "lyrics/rap/train/unpopular_rap/roc+marciano-burkina+faso.txt\n",
      "lyrics/rap/train/unpopular_rap/binary+star-i+know+why+the+caged+bird+sings+part+1.txt\n",
      "lyrics/rap/train/unpopular_rap/busdriver-casting+agents+and+cowgirls.txt\n",
      "lyrics/rap/train/unpopular_rap/roc+marciano-death+parade.txt\n",
      "lyrics/rap/train/unpopular_rap/aer-stars.txt\n",
      "lyrics/rap/train/unpopular_rap/roc+marciano-corniche.txt\n",
      "lyrics/rap/train/unpopular_rap/towkio-alone.txt\n",
      "lyrics/rap/train/unpopular_rap/roc+marciano-cvs.txt\n",
      "lyrics/rap/train/unpopular_rap/yonas-stand+out.txt\n",
      "lyrics/rap/train/unpopular_rap/chris+webby-blunt+ride+cypher.txt\n",
      "lyrics/rap/train/unpopular_rap/yonas-nobody+else.txt\n",
      "lyrics/rap/train/unpopular_rap/roc+marciano-confucius.txt\n",
      "lyrics/rap/train/unpopular_rap/binary+star-conquistadors.txt\n",
      "lyrics/rap/train/unpopular_rap/towkio-swim.txt\n",
      "lyrics/rap/train/unpopular_rap/oncue-this+is+not+a+song.txt\n",
      "lyrics/rap/train/unpopular_rap/roc+marciano-bruh+man.txt\n",
      "lyrics/rap/train/unpopular_rap/kevin+abstract-suburbian+born.txt\n",
      "lyrics/rap/train/unpopular_rap/chris+webby-through+the+roof.txt\n",
      "lyrics/rap/train/unpopular_rap/aer-im+not+sorry.txt\n",
      "lyrics/rap/train/unpopular_rap/blue+scholars-blink.txt\n",
      "lyrics/rap/train/unpopular_rap/roc+marciano-bedspring+king.txt\n",
      "lyrics/rap/train/unpopular_rap/aer-im+with+it.txt\n",
      "lyrics/rap/train/unpopular_rap/binary+star-solar+powered.txt\n",
      "lyrics/rap/train/unpopular_rap/nujabes-perfect+circle.txt\n",
      "lyrics/rap/train/unpopular_rap/yonas-mindless.txt\n",
      "lyrics/rap/train/unpopular_rap/kinetics+one+love-still+dreamin.txt\n",
      "lyrics/rap/train/unpopular_rap/towkio-tear+drop.txt\n",
      "lyrics/rap/train/unpopular_rap/hendersin-kids.txt\n",
      "lyrics/rap/train/unpopular_rap/roc+marciano-cut+the+check.txt\n",
      "lyrics/rap/train/unpopular_rap/chris+webby-house+party+cypher+interlude.txt\n",
      "lyrics/rap/train/unpopular_rap/chris+webby-nice+2+be+back.txt\n",
      "lyrics/rap/train/unpopular_rap/chris+webby-take+me+home.txt\n",
      "lyrics/rap/train/unpopular_rap/chris+webby-brim+low.txt\n",
      "lyrics/rap/train/unpopular_rap/roc+marciano-already.txt\n",
      "lyrics/rap/train/unpopular_rap/yonas-yall+know.txt\n",
      "lyrics/rap/train/unpopular_rap/binary+star-indy+500.txt\n",
      "lyrics/rap/train/unpopular_rap/chris+webby-label+office+cypher.txt\n",
      "lyrics/rap/train/unpopular_rap/chris+webby-stand+up.txt\n",
      "lyrics/rap/train/unpopular_rap/hendersin-fear.txt\n",
      "lyrics/rap/train/unpopular_rap/immortal+technique-crimes+of+the+heart.txt\n",
      "lyrics/rap/train/unpopular_rap/towkio-disco.txt\n",
      "lyrics/rap/train/unpopular_rap/roc+marciano-congo.txt\n",
      "lyrics/rap/train/unpopular_rap/roc+marciano-20+guns.txt\n",
      "lyrics/rap/train/unpopular_rap/busdriver-avantcore.txt\n",
      "lyrics/rap/train/unpopular_rap/towkio-hate+to+love.txt\n",
      "lyrics/rap/train/unpopular_rap/roc+marciano-chewbacca.txt\n",
      "lyrics/rap/train/unpopular_rap/tayyib+ali-i+want+it+all.txt\n",
      "lyrics/rap/train/unpopular_rap/binary+star-honest+expression.txt\n",
      "lyrics/rap/train/unpopular_rap/roc+marciano-love+means.txt\n",
      "lyrics/rap/train/unpopular_rap/aer-ex.txt\n",
      "lyrics/rap/train/unpopular_rap/roc+marciano-death+sentence.txt\n",
      "lyrics/rap/train/unpopular_rap/towkio-forever.txt\n",
      "lyrics/rap/train/unpopular_rap/nliten-lite+it+up.txt\n",
      "lyrics/rap/train/unpopular_rap/blue+scholars-burnt+offering.txt\n",
      "lyrics/rap/train/unpopular_rap/oncue-cereal.txt\n",
      "lyrics/rap/train/unpopular_rap/ollie-spring+is+here.txt\n",
      "lyrics/rap/train/unpopular_rap/blue+scholars-selfportrait.txt\n",
      "lyrics/rap/train/unpopular_rap/towkio-loose.txt\n",
      "lyrics/rap/train/unpopular_rap/binary+star-masters+of+the+universe.txt\n",
      "lyrics/rap/train/unpopular_rap/packy-wobbin.txt\n",
      "lyrics/rap/train/unpopular_rap/binary+star-evolution+of+man.txt\n",
      "lyrics/rap/train/unpopular_rap/busdriver-rapper.txt\n",
      "lyrics/rap/train/unpopular_rap/blue+scholars-life+debt.txt\n",
      "lyrics/rap/train/unpopular_rap/binary+star-glen+close.txt\n",
      "lyrics/rap/train/unpopular_rap/towkio-lose+me+i+dont+mind.txt\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import codecs\n",
    "from collections import Counter\n",
    "\n",
    "word_counts = Counter() # Counters are often useful for NLP in python\n",
    "\n",
    "corpus = ''\n",
    "for label in [RAP_POS_LABEL, RAP_NEG_LABEL]:\n",
    "    for directory in [RAP_TRAIN_DIR, RAP_TEST_DIR]:\n",
    "        for fn in glob.glob(directory + \"/\" + label + \"/*txt\"):\n",
    "            print(fn)\n",
    "            doc = codecs.open(fn, encoding = \"ISO-8859-1\") # Open the file with UTF-8 encoding\n",
    "            corpus += doc.read() + ' '\n",
    "    \n",
    "word_counts = Counter(tokenize_doc(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hmm. Something seems off. Double check your code\n"
     ]
    }
   ],
   "source": [
    "# you should see 61492 instances of the word type \"movie\" in the corpus. \n",
    "if word_counts[\"movie\"] == 61492:\n",
    "    print (\"yay! there are {} total instances of the word type movie in the corpus\".format(word_counts[\"movie\"]))\n",
    "else:\n",
    "    print (\"hmm. Something seems off. Double check your code\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.2 (5 points)**\n",
    "\n",
    "Take a look at the following values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 7418 word types in the corpus\n",
      "there are 58810 word tokens in the corpus\n"
     ]
    }
   ],
   "source": [
    "print (\"there are {} word types in the corpus\".format(n_word_types(word_counts)))\n",
    "print (\"there are {} word tokens in the corpus\".format(n_word_tokens(word_counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see a much higher number of tokens than types. Why is that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word types are the distinct words that appear in the corpus and the word tokens are the number of times each word type appears in the corpus. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.3 (5 points)**\n",
    "\n",
    "Using the word_counts dictionary you just created, make a new dictionary called sorted_dict where the words are sorted according to their counts, in decending order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "sorted_dict = sorted(word_counts.items(), key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now print the first 30 values from sorted_dict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the', 2486.0)\n",
      "('i', 2048.0)\n",
      "('you', 1411.0)\n",
      "('a', 1287.0)\n",
      "('to', 1205.0)\n",
      "('and', 976.0)\n",
      "('my', 887.0)\n",
      "('it', 826.0)\n",
      "('me', 820.0)\n",
      "('in', 801.0)\n",
      "('that', 653.0)\n",
      "(\"i'm\", 617.0)\n",
      "('on', 568.0)\n",
      "('up', 539.0)\n",
      "('of', 507.0)\n",
      "('like', 476.0)\n",
      "('we', 471.0)\n",
      "('with', 435.0)\n",
      "('got', 389.0)\n",
      "('your', 388.0)\n",
      "('but', 387.0)\n",
      "('is', 381.0)\n",
      "('all', 375.0)\n",
      "('so', 360.0)\n",
      "(\"don't\", 350.0)\n",
      "('for', 350.0)\n",
      "('get', 342.0)\n",
      "('this', 336.0)\n",
      "('no', 325.0)\n",
      "('be', 301.0)\n"
     ]
    }
   ],
   "source": [
    "for i in range(30):\n",
    "    print(sorted_dict[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zipf's Law\n",
    "\n",
    "**Question 1.4 (5 points)**\n",
    "\n",
    "In this section, you will verify a key statistical properties of text: [Zipf's Law](https://en.wikipedia.org/wiki/Zipf%27s_law).\n",
    "\n",
    "Zipf's Law describes the relations between the frequency rank of words and frequency value of words.  For a word $w$, its frequency is inversely proportional to its rank:\n",
    "\n",
    "$$count_w = K \\frac{1}{rank_w}$$\n",
    "or in other words\n",
    "$$\\log(count_w) = K - \\log(rank_w)$$\n",
    "\n",
    "for some constant $K$, specific to the corpus and how words are being defined.\n",
    "\n",
    "Therefore, if Zipf's Law holds, after sorting the words descending on frequency, word frequency decreases in an approximately linear fashion under a log-log scale.\n",
    "\n",
    "Please make such a log-log plot by ploting the rank versus frequency **Hint: Make use of the sorted dictionary you just created.**.  Use a scatter plot where the x-axis is the *log(rank)*, and y-axis is *log(frequency)*.  You should get this information from `word_counts`; for example, you can take the individual word counts and sort them.  dict methods `.items()` and/or `values()` may be useful.  (Note that it doesn't really matter whether ranks start at 1 or 0 in terms of how the plot comes out.) You can check your results by comparing your plots to ones on Wikipedia; they should look qualitatively similar.\n",
    "\n",
    "*Please remember to label the meaning of the x-axis and y-axis.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'log(frequency)')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "import operator\n",
    "x = []\n",
    "y = []\n",
    "X_LABEL = \"log(rank)\"\n",
    "Y_LABEL = \"log(frequency)\"\n",
    "\n",
    "for i in range(len(sorted_dict)):\n",
    "    x += [math.log(i+1)]\n",
    "    y += [math.log(sorted_dict[i][1])]\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.xlabel(X_LABEL)\n",
    "plt.ylabel(Y_LABEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.5 (5 points)**\n",
    "\n",
    "You should see some discontinuities on the left and right sides of this figure.  Why are we seeing them on the left?  Why are we seeing them on the right?  On the right, what are those \"ledges\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are seeing discontinuities on the left and right due to the nature of log growth. As x goes to 1 in log(x), the the growth of log(x) to log(x+1) is large. The ledges on the right are the infrequent words. There are many words that show up only once or twice in the text, and the large amounts of them create a ledge in the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Two: Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section of the homework will walk you through coding a Naive Bayes classifier that can distinguish between positive and negative reviews (at some level of accuracy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.1 (10 pts) ** \n",
    "\n",
    "To start, implement the `update_model` function in `hw1.py`. Make sure to read the function comments so you know what to update. Also review the NaiveBayes class variables in the `def __init__` method of the NaiveBayes class  to get a sense of which statistics are important to keep track of. Once you have implemented `update_model`, run the train model function using the code below. What is the size of the vocabulary used in the training documents? You’ll need to provide the path to the dataset you downloaded to run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPORTING CORPUS STATISTICS\n",
      "NUMBER OF DOCUMENTS IN POSITIVE CLASS: 38.0\n",
      "NUMBER OF DOCUMENTS IN NEGATIVE CLASS: 71.0\n",
      "NUMBER OF TOKENS IN POSITIVE CLASS: 19469.0\n",
      "NUMBER OF TOKENS IN NEGATIVE CLASS: 30684.0\n",
      "VOCABULARY SIZE: NUMBER OF UNIQUE WORDTYPES IN TRAINING CORPUS: 6884\n",
      "Oh no! Something seems off. Double check your code before continuing. Maybe a mistake in update_model?\n"
     ]
    }
   ],
   "source": [
    "nb = NaiveBayesTextClassification(PATH_TO_DATA, tokenizer=tokenize_doc)\n",
    "nb.train_model()\n",
    "\n",
    "\n",
    "if len(nb.vocab) == 251637:\n",
    "    print (\"Great! The vocabulary size is {}\".format(251637))\n",
    "else:\n",
    "    print (\"Oh no! Something seems off. Double check your code before continuing. Maybe a mistake in update_model?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploratory analysis\n",
    "\n",
    "Let’s begin to explore the count statistics stored by the update model function. Implement the provided `top_n` function to find the top 10 most common words in the positive class and top 10 most common words in the negative class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP 10 WORDS FOR CLASS popular_rap:\n",
      " i 676.0\n",
      " the 661.0\n",
      " a 481.0\n",
      " you 467.0\n",
      " to 361.0\n",
      " my 343.0\n",
      " me 296.0\n",
      " in 274.0\n",
      " and 247.0\n",
      " on 230.0\n",
      "\n",
      "TOP 10 WORDS FOR CLASS unpopular_rap:\n",
      " the 1420.0\n",
      " i 1070.0\n",
      " to 704.0\n",
      " you 701.0\n",
      " a 619.0\n",
      " and 553.0\n",
      " it 491.0\n",
      " me 442.0\n",
      " my 425.0\n",
      " in 396.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (\"TOP 10 WORDS FOR CLASS \" + RAP_POS_LABEL + \":\")\n",
    "for tok, count in nb.top_n(RAP_POS_LABEL, 10):\n",
    "    print ('', tok, count)\n",
    "print ()\n",
    "\n",
    "print (\"TOP 10 WORDS FOR CLASS \" + RAP_NEG_LABEL + \":\")\n",
    "for tok, count in nb.top_n(RAP_NEG_LABEL, 10):\n",
    "    print ('', tok, count)\n",
    "print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.2 (5 points)**\n",
    "\n",
    "What is the first thing that you notice when you look at the top 10 words for the 2 classes? Are these words helpful for discriminating between the two classes? Do you think this trend carries forward to other texts from the English language? What about other languages?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top 10 words for the 2 classes are practically identical. These words are not helpful for discriminating between the 2 classes. This trend carries to other texts in and out of the English language because there are many pronouns and prepositions that are commonly used throughtout all languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.3 (10 pts) **\n",
    "\n",
    "The Naive Bayes model assumes that all features are conditionally independent given the class label. For our purposes, this means that the probability of seeing a particular word in a document with class label $y$ is independent of the rest of the words in that document. Implement the `p_word_given_label` function. This function calculates P (w|y) (i.e., the probability of seeing word w in a document given the label of that document is y)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your `p_word_given_label` function to compute the probability of seeing the word “amazing” given each sentiment label. Repeat the computation for the word “dull.” "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P('amazing'|pos): 0.004982279521290257\n",
      "P('amazing'|neg): 0.001433972102724547\n",
      "P('dull'|pos): 0.0007704555960758128\n",
      "P('dull'|neg): 0.0008799374266718811\n"
     ]
    }
   ],
   "source": [
    "print (\"P('amazing'|pos):\",  nb.p_word_given_label(\"fuck\", RAP_POS_LABEL))\n",
    "print (\"P('amazing'|neg):\",  nb.p_word_given_label(\"fuck\", RAP_NEG_LABEL))\n",
    "print (\"P('dull'|pos):\",  nb.p_word_given_label(\"money\", RAP_POS_LABEL))\n",
    "print (\"P('dull'|neg):\",  nb.p_word_given_label(\"money\", RAP_NEG_LABEL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which word has a higher probability, given the positive class? Which word has a higher probability, given the negative class? Is this behavior expected?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the positive class, 'amazing' has the higher probability. Given the negative class, 'dull' has the higher probability. This is expected since 'amazing' is usually associated with positive connotations while 'dull' is assiciated with negative connotations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the purpose of the independence assumption for the Naive Bayes classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Naive Bayes classifier, we are not worried about the order of words and their relations with other words. Because of this, we can assume that probabilites for these words are all independent of each other. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.4 (5 pts)**\n",
    "\n",
    "In the next cell, compute the probability of the word \"stop-sign.\" in the positive training data and negative training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P('stop-sign.'|pos): 0.004982279521290257\n",
      "P('stop-sign.'|neg): 0.001433972102724547\n"
     ]
    }
   ],
   "source": [
    "print (\"P('stop-sign.'|pos):\",  nb.p_word_given_label(\"fuck\", RAP_POS_LABEL))\n",
    "print (\"P('stop-sign.'|neg):\",  nb.p_word_given_label(\"fuck\", RAP_NEG_LABEL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is unusual about P('stop-sign.'|pos)? Why is this a problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word 'stop-sign.' does not come up in the pos class. However, 'stop-sign.' is ambiguous and does not hold that much of a relation to a positive or negative review. Thus, by our current probability, see the word 'stop-sign.' would automatically make the review a negative review even though it shows low significance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.5 (5 pts)**\n",
    "\n",
    "We can address the issues from question 2.4 with add-$\\alpha$ smoothing (like add-1 smoothing except instead of adding 1 we add $\\alpha$). Implement\n",
    "`p_word_given_label_and_alpha` and then run the next cell. Hint: look at the slides from the lecture and the corresponding exercise on add-1 smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P('stop-sign.'|pos): 0.004662809774630862\n"
     ]
    }
   ],
   "source": [
    "print (\"P('stop-sign.'|pos):\",  nb.p_word_given_label_and_alpha(\"fuck\", RAP_POS_LABEL, 0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.6 (5 pts)** \n",
    "\n",
    "*Prior and Likelihood* \n",
    "\n",
    "As noted before, the Naive Bayes model assumes that all words in a document are independent of one another given the document’s label. Because of this we can write the likelihood of a document as:\n",
    "\n",
    "$P(w_{d1},\\cdots,w_{dn}|y_d) = \\prod_{i=1}^{n}P(w_{di}|y_d)$\n",
    "\n",
    "However, if a document has a lot of words, the likelihood will become extremely small and we’ll encounter numerical underflow. Underflow is a common problem when dealing with probabilistic models; if you are unfamiliar with it, you can get a brief overview on [Wikipedia](https:/en.wikipedia.org/wiki/Arithmetic_underflow). To deal with underflow, a common transformation is to work in log-space.\n",
    "\n",
    "$\\log[P(w_{d1},\\cdots,w_{dn}|y_d)] = \\sum_{i=1}^{n}\\log[P(w_{di}|y_d)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the log_likelihood function (Hint: it should make calls to the p word given label and alpha function). Implement the log_prior function. This function takes a class label and returns the log of the fraction of the training documents that are of that label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.7 (5 pts) **\n",
    "\n",
    "Naive Bayes is a model that tells us how to compute the posterior\n",
    "probability of a document being of some label (i.e.,\n",
    "$P(y_d|\\mathbf{w_d})$).  Specifically, we do so using bayes rule:\n",
    "\n",
    "  $P(y_d|\\mathbf{w_d}) = \\frac{P(y_d)P(\\mathbf{w_d}|y_d)}{P(\\mathbf{w_d})}$\n",
    "\n",
    "In the previous section you implemented functions to compute both\n",
    "the log prior ($\\log[P(y_d)]$) and the log likelihood\n",
    "($\\log[P( \\mathbf{w_d} |y_d)]$ ). Now, all you're missing is the\n",
    "*normalizer*, $P(\\mathbf{w_d})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derive the normalizer by expanding $P(\\mathbf{w_d})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(w_d) = \\prod_{i=0}^{n} P(w_{i})$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.8 (5 pts)**\n",
    "\n",
    "One way to classify a document is to compute the unnormalized log posterior for both labels and take the argmax (i.e., the label that yields the higher unnormalized log posterior). The unnormalized log posterior is the sum of the log prior and the log likelihood of the document. Why don’t we need to compute the log normalizer here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are calculating probabilites for individual words and not phrases, there are no dependencies for these probabilities. Additionally, order does not matter during out calculations. Thus, all words are independent of each toher and there is no need to normalize the log posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.9 (5 pts)**\n",
    "\n",
    "As we saw earlier, the top 10 words from each class do not give us much to go on when classifying a document. A much more powerful metric is the likelihood ratio, which is defined as\n",
    "\n",
    "$LR(w)=\\frac{P(w|y=\\mathrm{pos})}{P(w|y=\\mathrm{neg})}$\n",
    "\n",
    "A word with LR 3 is 3 times more likely to appear in the positive class than in the negative. A word with LR 0.3 is one-third as likely to appear in the positive class as opposed to the negative class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIKELIHOOD RATIO OF 'amazing': 3.382203882861654\n",
      "LIKELIHOOD RATIO OF 'dull': 39.9879496109528\n",
      "LIKELIHOOD RATIO OF 'and': 1.537998061959723\n",
      "LIKELIHOOD RATIO OF 'to': 3.113056318183536\n"
     ]
    }
   ],
   "source": [
    "# Implement the nb.likelihood_ratio function and use it to investigate the likelihood ratio of \"amazing\" and \"dull\"\n",
    "print (\"LIKELIHOOD RATIO OF 'amazing':\", nb.likelihood_ratio('fuck', 0.2))\n",
    "print (\"LIKELIHOOD RATIO OF 'dull':\", nb.likelihood_ratio('funk', 0.2))\n",
    "print (\"LIKELIHOOD RATIO OF 'and':\", nb.likelihood_ratio('police', 0.2))\n",
    "print (\"LIKELIHOOD RATIO OF 'to':\", nb.likelihood_ratio('her', 0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the minimum and maximum possible values the likelihood ratio can take?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The range for the likelihood ratio is (0, $\\infty$). The likelihood ratio will approach 0, but cannot be 0 because we are applying add-alpha smoothing to the probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the word in the vocabulary with the highest likelihood ratio below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bam 555.2173003674601\n"
     ]
    }
   ],
   "source": [
    "highest = -1\n",
    "word = None\n",
    "for n in nb.vocab:\n",
    "    temp = nb.likelihood_ratio(n, 0.2)\n",
    "    if temp > highest: \n",
    "        highest = temp\n",
    "        word = n\n",
    "print (word, highest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.10 (5 pts)**\n",
    "\n",
    "The unnormalized log posterior is the sum of the log prior and the log likelihood of the document. Implement the `unnormalized_log_posterior` function and the `classify` function. The `classify` function should use the unnormalized log posteriors but should not compute the normalizer. Once you implement the `classify` function, we'd like to evaluate its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.333333333333334\n"
     ]
    }
   ],
   "source": [
    "print (nb.evaluate_classifier_accuracy(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.11 (5 pts)**\n",
    "\n",
    "Try evaluating your model again with a smoothing parameter of 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print (nb.evaluate_classifier_accuracy(1000.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the accuracy go up or down when the pseudo count parameter is raised to 1000? Why do you think this is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy goes down when we raise the pseudo count paramater to 1000. As we raise the pseudo count parameter to 1000, we are slowly over-estimating for values that would normally have a low probability. By giving these values over-estimated probabilities, we are decreasing our overall accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Question 2.12 (5 pts)** \n",
    "\n",
    "Find a review that your classifier got wrong. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label for the review: popular_rap\n",
      "Classify_Label for the review: unpopular_rap\n",
      "Woke up quick at about noon Just thought that I had to be in compton soon I gotta get drunk before the day begin Before my mother starts bitchin' about my friends About to go and damn near went blind Young niggaz at the pad throwin' up gang signs Ran in the house and grabbed my clip With the mac-10 on the side of my hip Bailed outside and pointed my weapon Just as I thought the fools kept steppin' Jumped in the fo' hit the juice on my ride I got front 'nback and side to side Then I let the alpine play Bumpinnew shit by nwa It was \"gangsta gangsta\" at the top of the list Then I played my own shit it went somethin' like this: Cruisin' down the street in my 6 4 Jockin' the bitches slappin' the ho's Went to the park to get the scoop Knuckle-heads out there cold shootin' some hoops A car pulls up who can it be A fresh el camino rollinkilo g He rolls down his window and he started to say It's all about makin' that gta Cuz the boyz in the hood are always hard You come talkin' that trash we'll pull your card Knowin' nothin' in life but to be legit' Don't quote me boy cuz I ain't said shit... Dolla b'z in the place to give me the pace He said my man jb is on freebase The boy jb was a friend of mine 'till I caught him in my car tryin' to steal a alpine Chased him up the street to call a truce The silly mothafucka pull out a deuce-deuce Little did he know I had a loaded 12 gage One sucker dead L. A. Times front page Cuz the boyz in the hood are always hard You come talkin' that trash we'll pull your card Knowin' nothin' in life but to be legit' Don't quote me boy cuz I ain't said shit... Bored as hell and I wanna get ill So I went to a spot where my homeboys chill The fellows out there makin' that dollar I pulled up in my 6 4 impala They greet me with a 40 and I start drinkin' And from the 8-ball my breath start stinkin' Love to get my girl to rock that body Before I left I hit the bacardi Went to her house to get her out of the pad Dumb ho said something that made me mad She said somethin' that I couldn't believe So I grabbed the stupid bitch by her nappy ass weave She started talkin' shit wouldn't you know Reached back like a pimp and slapped the ho' Her father jumped out and he started to shout So I threw a right-cross and knocked his old ass out Cuz the boyz in the hood are always hard You come talkin' that trash we'll pull your card Knowin' nothin' in life but to be legit' Don't quote me boy cuz I ain't said shit... I'm rollin' hard now I'm under control Then wrapped the 6 4 'round the telephone poll I looked at my car and I said oh brother I throw it in the gutter and go buy another Walkin' home and I see the g ride Now kat is drivin' kilo on the side As they busted a you they got pulled over An undercover cop in a dark green nova Kat got beat for resistin' arrest He socked the pig in his head for rippin' his guess Now g is caught for doin' the crime For the fence on the boy he'll do some time Cuz the boyz in the hood are always hard You come talkin' that trash we'll pull your card Knowin' nothin' in life but to be legit' Don't quote me boy cuz I ain't said shit... I went to get the mouth but there was no bail The fellows start to riot in the county jail Two days later in municipal court Kilo g on trial cold cut a fart Disruption of a court said the judge On a six years sentence my man didn't budge Bailiff came over to turn him in Kilo g looked up and gave a grin He yelled out \"fire!\" then came suzi The bitch came in with a sub-machine uzi Police shot the bitch but didn't hurt her Both up state for attempted murder Cuz the boyz in the hood are always hard You come talkin' that trash we'll pull your card Knowin' nothin' in life but to be legit' Don't quote me boy cuz I ain't said shit... \n"
     ]
    }
   ],
   "source": [
    "alpha = 1.0\n",
    "pos_path = os.path.join(nb.rap_test_dir, RAP_POS_LABEL)\n",
    "neg_path = os.path.join(nb.rap_test_dir, RAP_NEG_LABEL)\n",
    "for (p, label) in [ (pos_path, RAP_POS_LABEL), (neg_path, RAP_NEG_LABEL) ]:\n",
    "    for f in os.listdir(p):\n",
    "        with open(os.path.join(p,f),'r') as doc:\n",
    "            content = doc.read()\n",
    "            bow = nb.tokenize_doc(content)\n",
    "            classify_label = nb.classify(bow, alpha)\n",
    "            if classify_label != label:\n",
    "                print ('Label for the review: {}'.format(label))\n",
    "                print ('Classify_Label for the review: {}'.format(classify_label))\n",
    "                print (content)\n",
    "                break\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are two reasons your system might have misclassified this example? What improvements could you make that may help your system classify this example correctly?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This review uses many words and phrases associated with positive connotations. Words like 'good', 'entertain', and 'potential' to name a few. Additionally, this review directs most of the positive statements to the actors, not the movie. Thus, many of these positive comments are geared towards the actors' abilites and not the writing of the movie. A way to improve our system to add context to words by looking at phrases or even whole sentences. This way the whole meaning of phrases and sentences can be understood instead of potentially misleading words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.13 (5 pts)**\n",
    "\n",
    "Often times we care about multi-class classification rather than binary classification.\n",
    "\n",
    "How many counts would we need to keep track of if the model were modified to support 5-class classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There would be 16 counts. 1 count for vocab, 5 for class_total_doc_counts, 5 for class_total_word_counts, and 5 for class_word_counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What would be the new decision rule (i.e., how would the classify function change)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of a binary classify function, there would need to be 5 checks for which class produced the maximum unnormalized_log_posterior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
